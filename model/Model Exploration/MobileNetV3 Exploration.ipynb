{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# MobileNet Architecture and Key Concepts\n",
        "\n",
        "## **What is MobileNet?**\n",
        "MobileNet is a family of lightweight deep learning models designed for mobile and embedded devices. It achieves efficiency and low computational cost through techniques like **depthwise separable convolutions**. These models are ideal for tasks requiring real-time predictions in resource-constrained environments.\n",
        "\n",
        "### **MobileNet Versions**\n",
        "1. **MobileNetV1**:\n",
        "   - Introduced depthwise separable convolutions to reduce computations.\n",
        "   - Lightweight but less accurate compared to newer versions.\n",
        "\n",
        "2. **MobileNetV2**:\n",
        "   - Added **inverted residual blocks** and **linear bottlenecks** for better feature extraction and efficiency.\n",
        "\n",
        "3. **MobileNetV3**:\n",
        "   - Combines ideas from **MobileNetV2** and **Neural Architecture Search (NAS)**.\n",
        "   - Includes:\n",
        "     - **Squeeze-and-Excitation (SE) Blocks** for channel-wise attention.\n",
        "     - **Hard-Swish Activation** for improved gradients and efficiency.\n",
        "     - **Small** and **Large** variants tailored for different resource requirements.\n",
        "\n",
        "---\n",
        "\n",
        "## **Key Architectural Features**\n",
        "\n",
        "### **Depthwise Separable Convolutions**\n",
        "- **Depthwise Convolution**: Applies one filter per input channel to reduce computation.\n",
        "- **Pointwise Convolution**: Uses 1x1 convolutions to combine outputs from depthwise convolutions.\n",
        "\n",
        "### **Inverted Residuals and Linear Bottlenecks** (MobileNetV2 and V3)\n",
        "- **Inverted Residuals**: Shortcut connections between thin bottleneck layers.\n",
        "- **Linear Bottleneck**: Prevents information loss by using linear activation instead of ReLU in bottleneck layers.\n",
        "\n",
        "### **Squeeze-and-Excitation (SE) Blocks**\n",
        "- Channel-wise attention mechanism that learns to focus on important features while ignoring irrelevant ones.\n",
        "\n",
        "### **Hard-Swish Activation**\n",
        "- A computationally efficient alternative to standard Swish activation, improving model performance with minimal cost.\n",
        "\n",
        "---\n",
        "\n",
        "## **Weights and Layers in MobileNet**\n",
        "\n",
        "### **How Weights Work in MobileNet**\n",
        "- MobileNet uses pre-trained weights when loaded with `weights='imagenet'`. These weights are optimized for recognizing features in ImageNet images, such as edges, textures, and shapes.\n",
        "- The pre-trained weights are found in trainable layers (e.g., convolutional layers).\n",
        "\n",
        "### **Accessing Weights in Layers**\n",
        "1. **Inspecting Layers**:\n",
        "   - Use a loop to inspect all layers in the model and identify which ones contain weights:\n",
        "     ```python\n",
        "     for i, layer in enumerate(base_model.layers):\n",
        "         print(i, layer.name, type(layer), len(layer.weights))\n",
        "     ```\n",
        "\n",
        "2. **Extracting Weights**:\n",
        "   - Locate a specific layer and retrieve its weights and biases:\n",
        "     ```python\n",
        "     conv_layer = base_model.layers[1]  # Replace with correct index\n",
        "     weights, biases = conv_layer.get_weights()\n",
        "     print(\"Weights shape:\", weights.shape)\n",
        "     print(\"Biases shape:\", biases.shape)\n",
        "     ```\n",
        "\n",
        "3. **Handling Empty Weights**:\n",
        "   - Layers like input or preprocessing layers do not have trainable weights, so `get_weights()` may return an empty list.\n",
        "   - Ensure the model is initialized by passing a dummy input:\n",
        "     ```python\n",
        "     dummy_input = tf.random.normal([1, 224, 224, 3])\n",
        "     base_model(dummy_input)  # Forward pass to build the model\n",
        "     ```\n",
        "\n",
        "### **Impact of Replacing Non-Trainable Layers**\n",
        "- Non-trainable layers, such as input or activation layers, do not significantly affect training as they lack learnable parameters.\n",
        "- Replacing them is generally safe but ensure downstream layers can handle the changes (e.g., modified input shapes).\n",
        "\n",
        "### **When to Modify Weights**\n",
        "- Modify trainable layers when adapting the model to new tasks (e.g., handling 5-channel input: RGB + mask + texture).\n",
        "- Ensure new weights for additional channels are initialized properly (e.g., using random initialization) and fine-tuned during training.\n",
        "\n",
        "---\n",
        "\n",
        "## **Adapting MobileNet for 5-Channel Input**\n",
        "\n",
        "### **Problem**\n",
        "Standard MobileNet expects 3-channel input (RGB). To handle 5-channel input (e.g., RGB + mask + texture):\n",
        "- Modify the first convolutional layer to accept 5 channels.\n",
        "\n",
        "### **Steps to Modify**\n",
        "1. **Inspect the Model**:\n",
        "   - Identify the first convolutional layer with weights.\n",
        "\n",
        "2. **Extend Weights**:\n",
        "   - Add random weights for the additional channels:\n",
        "     ```python\n",
        "     new_weights = tf.concat([weights, tf.random.normal([3, 3, 2, weights.shape[-1]])], axis=2)\n",
        "     ```\n",
        "\n",
        "3. **Replace the Layer**:\n",
        "   - Create a new input layer and attach the modified convolutional layer to the model.\n",
        "\n",
        "4. **Fine-Tune the Model**:\n",
        "   - Train the model on your dataset to adapt it to the new input format.\n",
        "\n",
        "---\n",
        "\n",
        "## **Conclusion**\n",
        "- MobileNet is highly flexible and efficient, making it ideal for mobile and embedded applications.\n",
        "- Pre-trained weights can be adapted to new tasks by modifying the architecture.\n",
        "- Replacing non-trainable layers has minimal impact, but ensure compatibility with downstream layers.\n",
        "- When adding new input channels, extend the first layer's weights and fine-tune the model for optimal performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "Aw_Kkse5MB1_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import models\n",
        "from torchvision.models import MobileNet_V3_Large_Weights\n",
        "from torch.nn import Conv2d\n",
        "\n",
        "# Load the pre-trained MobileNetV3 model\n",
        "mobilenet = models.mobilenet_v3_large(weights=MobileNet_V3_Large_Weights.IMAGENET1K_V1)\n",
        "\n",
        "# Extract the first convolutional layer properties\n",
        "first_conv_layer = mobilenet.features[0][0]\n",
        "\n",
        "# Create a new Conv2d layer with 5 input channels instead of 3\n",
        "new_first_conv_layer = Conv2d(\n",
        "    in_channels=5,\n",
        "    out_channels=first_conv_layer.out_channels,\n",
        "    kernel_size=first_conv_layer.kernel_size,\n",
        "    stride=first_conv_layer.stride,\n",
        "    padding=first_conv_layer.padding,\n",
        "    bias=(first_conv_layer.bias is not None)  # Preserve the use of bias if it was used\n",
        ")\n",
        "\n",
        "# Initialize the new convolutional layer weights\n",
        "with torch.no_grad():\n",
        "    # Copy weights from the first three channels\n",
        "    new_first_conv_layer.weight[:, :3, :, :] = first_conv_layer.weight.clone()\n",
        "\n",
        "    # Initialize the weights for the two new channels by averaging the original three channels\n",
        "    new_channel_weights = first_conv_layer.weight.mean(dim=1, keepdim=True).expand(-1, 2, -1, -1)\n",
        "    new_first_conv_layer.weight[:, 3:5, :, :] = new_channel_weights\n",
        "\n",
        "# Replace the first conv layer in the model with the new layer\n",
        "mobilenet.features[0][0] = new_first_conv_layer\n",
        "\n",
        "# Verify the changes by printing the model summary or a specific part\n",
        "print(mobilenet.features[0])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7ivIR28fKwu",
        "outputId": "b1a8f882-049f-41ca-b326-c50cf4e6cd7d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/mobilenet_v3_large-8738ca79.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v3_large-8738ca79.pth\n",
            "100%|██████████| 21.1M/21.1M [00:00<00:00, 109MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conv2dNormActivation(\n",
            "  (0): Conv2d(5, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "  (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
            "  (2): Hardswish()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "summary(mobilenet, (5, 224, 224))\n"
      ],
      "metadata": {
        "id": "KyZCVbkMfrrx",
        "outputId": "edcbaae4-d2df-4aec-c901-48e3191879b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 16, 112, 112]             720\n",
            "       BatchNorm2d-2         [-1, 16, 112, 112]              32\n",
            "         Hardswish-3         [-1, 16, 112, 112]               0\n",
            "            Conv2d-4         [-1, 16, 112, 112]             144\n",
            "       BatchNorm2d-5         [-1, 16, 112, 112]              32\n",
            "              ReLU-6         [-1, 16, 112, 112]               0\n",
            "            Conv2d-7         [-1, 16, 112, 112]             256\n",
            "       BatchNorm2d-8         [-1, 16, 112, 112]              32\n",
            "  InvertedResidual-9         [-1, 16, 112, 112]               0\n",
            "           Conv2d-10         [-1, 64, 112, 112]           1,024\n",
            "      BatchNorm2d-11         [-1, 64, 112, 112]             128\n",
            "             ReLU-12         [-1, 64, 112, 112]               0\n",
            "           Conv2d-13           [-1, 64, 56, 56]             576\n",
            "      BatchNorm2d-14           [-1, 64, 56, 56]             128\n",
            "             ReLU-15           [-1, 64, 56, 56]               0\n",
            "           Conv2d-16           [-1, 24, 56, 56]           1,536\n",
            "      BatchNorm2d-17           [-1, 24, 56, 56]              48\n",
            " InvertedResidual-18           [-1, 24, 56, 56]               0\n",
            "           Conv2d-19           [-1, 72, 56, 56]           1,728\n",
            "      BatchNorm2d-20           [-1, 72, 56, 56]             144\n",
            "             ReLU-21           [-1, 72, 56, 56]               0\n",
            "           Conv2d-22           [-1, 72, 56, 56]             648\n",
            "      BatchNorm2d-23           [-1, 72, 56, 56]             144\n",
            "             ReLU-24           [-1, 72, 56, 56]               0\n",
            "           Conv2d-25           [-1, 24, 56, 56]           1,728\n",
            "      BatchNorm2d-26           [-1, 24, 56, 56]              48\n",
            " InvertedResidual-27           [-1, 24, 56, 56]               0\n",
            "           Conv2d-28           [-1, 72, 56, 56]           1,728\n",
            "      BatchNorm2d-29           [-1, 72, 56, 56]             144\n",
            "             ReLU-30           [-1, 72, 56, 56]               0\n",
            "           Conv2d-31           [-1, 72, 28, 28]           1,800\n",
            "      BatchNorm2d-32           [-1, 72, 28, 28]             144\n",
            "             ReLU-33           [-1, 72, 28, 28]               0\n",
            "AdaptiveAvgPool2d-34             [-1, 72, 1, 1]               0\n",
            "           Conv2d-35             [-1, 24, 1, 1]           1,752\n",
            "             ReLU-36             [-1, 24, 1, 1]               0\n",
            "           Conv2d-37             [-1, 72, 1, 1]           1,800\n",
            "      Hardsigmoid-38             [-1, 72, 1, 1]               0\n",
            "SqueezeExcitation-39           [-1, 72, 28, 28]               0\n",
            "           Conv2d-40           [-1, 40, 28, 28]           2,880\n",
            "      BatchNorm2d-41           [-1, 40, 28, 28]              80\n",
            " InvertedResidual-42           [-1, 40, 28, 28]               0\n",
            "           Conv2d-43          [-1, 120, 28, 28]           4,800\n",
            "      BatchNorm2d-44          [-1, 120, 28, 28]             240\n",
            "             ReLU-45          [-1, 120, 28, 28]               0\n",
            "           Conv2d-46          [-1, 120, 28, 28]           3,000\n",
            "      BatchNorm2d-47          [-1, 120, 28, 28]             240\n",
            "             ReLU-48          [-1, 120, 28, 28]               0\n",
            "AdaptiveAvgPool2d-49            [-1, 120, 1, 1]               0\n",
            "           Conv2d-50             [-1, 32, 1, 1]           3,872\n",
            "             ReLU-51             [-1, 32, 1, 1]               0\n",
            "           Conv2d-52            [-1, 120, 1, 1]           3,960\n",
            "      Hardsigmoid-53            [-1, 120, 1, 1]               0\n",
            "SqueezeExcitation-54          [-1, 120, 28, 28]               0\n",
            "           Conv2d-55           [-1, 40, 28, 28]           4,800\n",
            "      BatchNorm2d-56           [-1, 40, 28, 28]              80\n",
            " InvertedResidual-57           [-1, 40, 28, 28]               0\n",
            "           Conv2d-58          [-1, 120, 28, 28]           4,800\n",
            "      BatchNorm2d-59          [-1, 120, 28, 28]             240\n",
            "             ReLU-60          [-1, 120, 28, 28]               0\n",
            "           Conv2d-61          [-1, 120, 28, 28]           3,000\n",
            "      BatchNorm2d-62          [-1, 120, 28, 28]             240\n",
            "             ReLU-63          [-1, 120, 28, 28]               0\n",
            "AdaptiveAvgPool2d-64            [-1, 120, 1, 1]               0\n",
            "           Conv2d-65             [-1, 32, 1, 1]           3,872\n",
            "             ReLU-66             [-1, 32, 1, 1]               0\n",
            "           Conv2d-67            [-1, 120, 1, 1]           3,960\n",
            "      Hardsigmoid-68            [-1, 120, 1, 1]               0\n",
            "SqueezeExcitation-69          [-1, 120, 28, 28]               0\n",
            "           Conv2d-70           [-1, 40, 28, 28]           4,800\n",
            "      BatchNorm2d-71           [-1, 40, 28, 28]              80\n",
            " InvertedResidual-72           [-1, 40, 28, 28]               0\n",
            "           Conv2d-73          [-1, 240, 28, 28]           9,600\n",
            "      BatchNorm2d-74          [-1, 240, 28, 28]             480\n",
            "        Hardswish-75          [-1, 240, 28, 28]               0\n",
            "           Conv2d-76          [-1, 240, 14, 14]           2,160\n",
            "      BatchNorm2d-77          [-1, 240, 14, 14]             480\n",
            "        Hardswish-78          [-1, 240, 14, 14]               0\n",
            "           Conv2d-79           [-1, 80, 14, 14]          19,200\n",
            "      BatchNorm2d-80           [-1, 80, 14, 14]             160\n",
            " InvertedResidual-81           [-1, 80, 14, 14]               0\n",
            "           Conv2d-82          [-1, 200, 14, 14]          16,000\n",
            "      BatchNorm2d-83          [-1, 200, 14, 14]             400\n",
            "        Hardswish-84          [-1, 200, 14, 14]               0\n",
            "           Conv2d-85          [-1, 200, 14, 14]           1,800\n",
            "      BatchNorm2d-86          [-1, 200, 14, 14]             400\n",
            "        Hardswish-87          [-1, 200, 14, 14]               0\n",
            "           Conv2d-88           [-1, 80, 14, 14]          16,000\n",
            "      BatchNorm2d-89           [-1, 80, 14, 14]             160\n",
            " InvertedResidual-90           [-1, 80, 14, 14]               0\n",
            "           Conv2d-91          [-1, 184, 14, 14]          14,720\n",
            "      BatchNorm2d-92          [-1, 184, 14, 14]             368\n",
            "        Hardswish-93          [-1, 184, 14, 14]               0\n",
            "           Conv2d-94          [-1, 184, 14, 14]           1,656\n",
            "      BatchNorm2d-95          [-1, 184, 14, 14]             368\n",
            "        Hardswish-96          [-1, 184, 14, 14]               0\n",
            "           Conv2d-97           [-1, 80, 14, 14]          14,720\n",
            "      BatchNorm2d-98           [-1, 80, 14, 14]             160\n",
            " InvertedResidual-99           [-1, 80, 14, 14]               0\n",
            "          Conv2d-100          [-1, 184, 14, 14]          14,720\n",
            "     BatchNorm2d-101          [-1, 184, 14, 14]             368\n",
            "       Hardswish-102          [-1, 184, 14, 14]               0\n",
            "          Conv2d-103          [-1, 184, 14, 14]           1,656\n",
            "     BatchNorm2d-104          [-1, 184, 14, 14]             368\n",
            "       Hardswish-105          [-1, 184, 14, 14]               0\n",
            "          Conv2d-106           [-1, 80, 14, 14]          14,720\n",
            "     BatchNorm2d-107           [-1, 80, 14, 14]             160\n",
            "InvertedResidual-108           [-1, 80, 14, 14]               0\n",
            "          Conv2d-109          [-1, 480, 14, 14]          38,400\n",
            "     BatchNorm2d-110          [-1, 480, 14, 14]             960\n",
            "       Hardswish-111          [-1, 480, 14, 14]               0\n",
            "          Conv2d-112          [-1, 480, 14, 14]           4,320\n",
            "     BatchNorm2d-113          [-1, 480, 14, 14]             960\n",
            "       Hardswish-114          [-1, 480, 14, 14]               0\n",
            "AdaptiveAvgPool2d-115            [-1, 480, 1, 1]               0\n",
            "          Conv2d-116            [-1, 120, 1, 1]          57,720\n",
            "            ReLU-117            [-1, 120, 1, 1]               0\n",
            "          Conv2d-118            [-1, 480, 1, 1]          58,080\n",
            "     Hardsigmoid-119            [-1, 480, 1, 1]               0\n",
            "SqueezeExcitation-120          [-1, 480, 14, 14]               0\n",
            "          Conv2d-121          [-1, 112, 14, 14]          53,760\n",
            "     BatchNorm2d-122          [-1, 112, 14, 14]             224\n",
            "InvertedResidual-123          [-1, 112, 14, 14]               0\n",
            "          Conv2d-124          [-1, 672, 14, 14]          75,264\n",
            "     BatchNorm2d-125          [-1, 672, 14, 14]           1,344\n",
            "       Hardswish-126          [-1, 672, 14, 14]               0\n",
            "          Conv2d-127          [-1, 672, 14, 14]           6,048\n",
            "     BatchNorm2d-128          [-1, 672, 14, 14]           1,344\n",
            "       Hardswish-129          [-1, 672, 14, 14]               0\n",
            "AdaptiveAvgPool2d-130            [-1, 672, 1, 1]               0\n",
            "          Conv2d-131            [-1, 168, 1, 1]         113,064\n",
            "            ReLU-132            [-1, 168, 1, 1]               0\n",
            "          Conv2d-133            [-1, 672, 1, 1]         113,568\n",
            "     Hardsigmoid-134            [-1, 672, 1, 1]               0\n",
            "SqueezeExcitation-135          [-1, 672, 14, 14]               0\n",
            "          Conv2d-136          [-1, 112, 14, 14]          75,264\n",
            "     BatchNorm2d-137          [-1, 112, 14, 14]             224\n",
            "InvertedResidual-138          [-1, 112, 14, 14]               0\n",
            "          Conv2d-139          [-1, 672, 14, 14]          75,264\n",
            "     BatchNorm2d-140          [-1, 672, 14, 14]           1,344\n",
            "       Hardswish-141          [-1, 672, 14, 14]               0\n",
            "          Conv2d-142            [-1, 672, 7, 7]          16,800\n",
            "     BatchNorm2d-143            [-1, 672, 7, 7]           1,344\n",
            "       Hardswish-144            [-1, 672, 7, 7]               0\n",
            "AdaptiveAvgPool2d-145            [-1, 672, 1, 1]               0\n",
            "          Conv2d-146            [-1, 168, 1, 1]         113,064\n",
            "            ReLU-147            [-1, 168, 1, 1]               0\n",
            "          Conv2d-148            [-1, 672, 1, 1]         113,568\n",
            "     Hardsigmoid-149            [-1, 672, 1, 1]               0\n",
            "SqueezeExcitation-150            [-1, 672, 7, 7]               0\n",
            "          Conv2d-151            [-1, 160, 7, 7]         107,520\n",
            "     BatchNorm2d-152            [-1, 160, 7, 7]             320\n",
            "InvertedResidual-153            [-1, 160, 7, 7]               0\n",
            "          Conv2d-154            [-1, 960, 7, 7]         153,600\n",
            "     BatchNorm2d-155            [-1, 960, 7, 7]           1,920\n",
            "       Hardswish-156            [-1, 960, 7, 7]               0\n",
            "          Conv2d-157            [-1, 960, 7, 7]          24,000\n",
            "     BatchNorm2d-158            [-1, 960, 7, 7]           1,920\n",
            "       Hardswish-159            [-1, 960, 7, 7]               0\n",
            "AdaptiveAvgPool2d-160            [-1, 960, 1, 1]               0\n",
            "          Conv2d-161            [-1, 240, 1, 1]         230,640\n",
            "            ReLU-162            [-1, 240, 1, 1]               0\n",
            "          Conv2d-163            [-1, 960, 1, 1]         231,360\n",
            "     Hardsigmoid-164            [-1, 960, 1, 1]               0\n",
            "SqueezeExcitation-165            [-1, 960, 7, 7]               0\n",
            "          Conv2d-166            [-1, 160, 7, 7]         153,600\n",
            "     BatchNorm2d-167            [-1, 160, 7, 7]             320\n",
            "InvertedResidual-168            [-1, 160, 7, 7]               0\n",
            "          Conv2d-169            [-1, 960, 7, 7]         153,600\n",
            "     BatchNorm2d-170            [-1, 960, 7, 7]           1,920\n",
            "       Hardswish-171            [-1, 960, 7, 7]               0\n",
            "          Conv2d-172            [-1, 960, 7, 7]          24,000\n",
            "     BatchNorm2d-173            [-1, 960, 7, 7]           1,920\n",
            "       Hardswish-174            [-1, 960, 7, 7]               0\n",
            "AdaptiveAvgPool2d-175            [-1, 960, 1, 1]               0\n",
            "          Conv2d-176            [-1, 240, 1, 1]         230,640\n",
            "            ReLU-177            [-1, 240, 1, 1]               0\n",
            "          Conv2d-178            [-1, 960, 1, 1]         231,360\n",
            "     Hardsigmoid-179            [-1, 960, 1, 1]               0\n",
            "SqueezeExcitation-180            [-1, 960, 7, 7]               0\n",
            "          Conv2d-181            [-1, 160, 7, 7]         153,600\n",
            "     BatchNorm2d-182            [-1, 160, 7, 7]             320\n",
            "InvertedResidual-183            [-1, 160, 7, 7]               0\n",
            "          Conv2d-184            [-1, 960, 7, 7]         153,600\n",
            "     BatchNorm2d-185            [-1, 960, 7, 7]           1,920\n",
            "       Hardswish-186            [-1, 960, 7, 7]               0\n",
            "AdaptiveAvgPool2d-187            [-1, 960, 1, 1]               0\n",
            "          Linear-188                 [-1, 1280]       1,230,080\n",
            "       Hardswish-189                 [-1, 1280]               0\n",
            "         Dropout-190                 [-1, 1280]               0\n",
            "          Linear-191                 [-1, 1000]       1,281,000\n",
            "================================================================\n",
            "Total params: 5,483,320\n",
            "Trainable params: 5,483,320\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.96\n",
            "Forward/backward pass size (MB): 105.41\n",
            "Params size (MB): 20.92\n",
            "Estimated Total Size (MB): 127.29\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OA3DyEbowzrW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}